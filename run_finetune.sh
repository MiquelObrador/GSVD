python alpaca_finetune.py \
    --model-path "results/llama7b/gsvd_hmsecos/gsvd_llama-7b_r0.6_g50_cb10.pt" \
    --output-dir "finetuned_models" \
    --base-model "huggyllama/llama-7b" \
    --dataset "yahma/alpaca-cleaned" \
    --num-epochs 1 \
    --micro-batch-size 4 \
    --gradient-accumulation-steps 16 \
    --learning-rate 1e-4 \
    --cutoff-len 256 \
    --lora-r 8 \
    --lora-alpha 16 \
    --lora-dropout 0.05 \
    --save-steps 200 \
    --eval-steps 100
