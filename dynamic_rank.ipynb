{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dc8bcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from collections import OrderedDict\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "\n",
    "# 1) Load model & freeze weights\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"huggyllama/llama-7b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5988f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from collections import OrderedDict\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# Load the gradients to verify\n",
    "with open(\"grads/llama7b_grads_out.pt\", \"rb\") as f:\n",
    "    importance_dict = torch.load(f)\n",
    "\n",
    "# Calculate the average importance of each layer\n",
    "importance_avg = OrderedDict()\n",
    "for layer_name, importance in importance_dict.items():\n",
    "    importance_avg[layer_name] = torch.mean(importance).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1828ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: huggyllama/llama-7b. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rank calculation...\n",
      "Found 224 compressible layers.\n",
      "Total original parameters in these layers: 6,476,005,376\n",
      "Target parameters after compression: 3,885,603,225\n",
      "\n",
      "--- Compression Results ---\n",
      "Target Compression Ratio: 60.00%\n",
      "Achieved Compression Ratio: 40.01%\n",
      "Original Parameters: 6,476,005,376\n",
      "Final Parameters: 3,885,260,800\n",
      "---------------------------\n",
      "\n",
      "Layer Name                               Importance   Shape           Orig. Params    New Rank (k)    New Params      Compression\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "model.layers.31.self_attn.o_proj.weight  0.6777       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.30.mlp.down_proj.weight     0.4827       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.12.self_attn.v_proj.weight  0.4106       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.14.self_attn.v_proj.weight  0.3994       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.25.self_attn.v_proj.weight  0.3965       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.26.self_attn.o_proj.weight  0.3892       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.29.mlp.down_proj.weight     0.3877       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.31.mlp.down_proj.weight     0.3867       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.30.self_attn.o_proj.weight  0.3828       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.5.self_attn.v_proj.weight   0.3801       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.27.self_attn.o_proj.weight  0.3796       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.29.self_attn.v_proj.weight  0.3787       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.13.self_attn.v_proj.weight  0.3779       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.25.self_attn.o_proj.weight  0.3726       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.28.self_attn.o_proj.weight  0.3726       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.26.mlp.down_proj.weight     0.3716       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.27.mlp.down_proj.weight     0.3713       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.3.self_attn.v_proj.weight   0.3708       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.23.self_attn.o_proj.weight  0.3694       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.24.self_attn.o_proj.weight  0.3667       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.28.mlp.down_proj.weight     0.3667       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.17.self_attn.v_proj.weight  0.3647       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.29.self_attn.o_proj.weight  0.3640       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.23.self_attn.v_proj.weight  0.3621       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.18.self_attn.v_proj.weight  0.3608       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.26.self_attn.v_proj.weight  0.3594       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.25.mlp.down_proj.weight     0.3562       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.24.mlp.down_proj.weight     0.3535       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.23.mlp.down_proj.weight     0.3499       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.15.self_attn.v_proj.weight  0.3481       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.9.self_attn.v_proj.weight   0.3411       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.28.self_attn.v_proj.weight  0.3291       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.10.self_attn.v_proj.weight  0.3220       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.21.self_attn.v_proj.weight  0.3206       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.22.self_attn.o_proj.weight  0.3164       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.21.self_attn.o_proj.weight  0.3162       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.19.self_attn.o_proj.weight  0.3115       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.20.self_attn.o_proj.weight  0.3083       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.16.self_attn.v_proj.weight  0.3062       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.2.self_attn.v_proj.weight   0.2988       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.21.mlp.down_proj.weight     0.2983       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.27.self_attn.v_proj.weight  0.2969       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.22.mlp.down_proj.weight     0.2966       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.7.self_attn.v_proj.weight   0.2961       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.19.mlp.down_proj.weight     0.2947       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.18.self_attn.o_proj.weight  0.2932       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.20.self_attn.v_proj.weight  0.2932       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.20.mlp.down_proj.weight     0.2896       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.18.mlp.down_proj.weight     0.2742       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.6.mlp.up_proj.weight        0.2725       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.16.self_attn.o_proj.weight  0.2708       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.17.self_attn.o_proj.weight  0.2681       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.17.mlp.down_proj.weight     0.2634       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.24.self_attn.v_proj.weight  0.2622       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.19.self_attn.v_proj.weight  0.2607       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.15.self_attn.o_proj.weight  0.2603       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.15.mlp.down_proj.weight     0.2549       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.16.mlp.down_proj.weight     0.2507       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.14.mlp.down_proj.weight     0.2499       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.13.self_attn.o_proj.weight  0.2483       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.12.self_attn.o_proj.weight  0.2469       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.12.mlp.down_proj.weight     0.2454       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.20.mlp.gate_proj.weight     0.2439       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.14.self_attn.o_proj.weight  0.2435       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.13.mlp.down_proj.weight     0.2423       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.11.self_attn.o_proj.weight  0.2413       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.11.mlp.down_proj.weight     0.2383       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.14.mlp.gate_proj.weight     0.2366       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.10.mlp.gate_proj.weight     0.2310       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.8.self_attn.o_proj.weight   0.2255       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.2.self_attn.o_proj.weight   0.2246       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.3.mlp.up_proj.weight        0.2184       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.4.self_attn.o_proj.weight   0.2181       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.15.mlp.gate_proj.weight     0.2169       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.8.mlp.down_proj.weight      0.2167       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.5.self_attn.o_proj.weight   0.2162       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.3.self_attn.o_proj.weight   0.2151       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.7.self_attn.o_proj.weight   0.2128       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.7.mlp.down_proj.weight      0.2128       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.11.self_attn.v_proj.weight  0.2120       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.1.self_attn.o_proj.weight   0.2118       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.9.self_attn.o_proj.weight   0.2113       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.10.mlp.down_proj.weight     0.2100       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.1.mlp.down_proj.weight      0.2081       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.4.mlp.down_proj.weight      0.2079       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.8.mlp.gate_proj.weight      0.2079       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.0.mlp.down_proj.weight      0.2076       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.6.self_attn.o_proj.weight   0.2061       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.2.mlp.gate_proj.weight      0.2042       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.12.mlp.gate_proj.weight     0.2039       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.10.self_attn.o_proj.weight  0.2031       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.9.mlp.down_proj.weight      0.2024       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.22.self_attn.v_proj.weight  0.2017       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.21.mlp.gate_proj.weight     0.2010       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.30.self_attn.v_proj.weight  0.2002       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.2.mlp.down_proj.weight      0.2000       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.5.mlp.down_proj.weight      0.1973       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.9.mlp.gate_proj.weight      0.1957       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.6.mlp.down_proj.weight      0.1942       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.3.mlp.down_proj.weight      0.1912       (4096, 11008)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.16.mlp.gate_proj.weight     0.1904       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.24.mlp.gate_proj.weight     0.1849       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.3.mlp.gate_proj.weight      0.1838       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.18.mlp.gate_proj.weight     0.1779       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.2.mlp.up_proj.weight        0.1696       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.11.mlp.gate_proj.weight     0.1638       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.19.mlp.gate_proj.weight     0.1603       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.4.mlp.up_proj.weight        0.1577       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.13.mlp.gate_proj.weight     0.1554       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.25.mlp.gate_proj.weight     0.1470       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.23.mlp.gate_proj.weight     0.1454       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.6.self_attn.v_proj.weight   0.1448       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.22.mlp.gate_proj.weight     0.1443       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.31.self_attn.v_proj.weight  0.1425       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.13.mlp.up_proj.weight       0.1420       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.5.mlp.up_proj.weight        0.1415       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.1.mlp.gate_proj.weight      0.1410       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.1.self_attn.v_proj.weight   0.1339       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.23.mlp.up_proj.weight       0.1324       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.19.mlp.up_proj.weight       0.1242       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.11.mlp.up_proj.weight       0.1232       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.18.mlp.up_proj.weight       0.1228       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.21.mlp.up_proj.weight       0.1216       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.20.mlp.up_proj.weight       0.1188       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.25.mlp.up_proj.weight       0.1182       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.8.mlp.up_proj.weight        0.1168       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.7.mlp.up_proj.weight        0.1138       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.12.mlp.up_proj.weight       0.1133       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.0.self_attn.o_proj.weight   0.1127       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.0.self_attn.v_proj.weight   0.1096       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.4.self_attn.v_proj.weight   0.1083       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.22.mlp.up_proj.weight       0.1060       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.17.mlp.gate_proj.weight     0.1060       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.6.mlp.gate_proj.weight      0.1057       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.15.mlp.up_proj.weight       0.1046       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.1.mlp.up_proj.weight        0.1041       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.8.self_attn.v_proj.weight   0.1041       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.24.mlp.up_proj.weight       0.1000       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.26.mlp.gate_proj.weight     0.1000       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.27.mlp.up_proj.weight       0.0995       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.27.mlp.gate_proj.weight     0.0991       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.16.mlp.up_proj.weight       0.0969       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.9.mlp.up_proj.weight        0.0954       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.5.mlp.gate_proj.weight      0.0894       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.26.mlp.up_proj.weight       0.0883       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.28.mlp.gate_proj.weight     0.0853       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.12.self_attn.q_proj.weight  0.0841       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.13.self_attn.q_proj.weight  0.0813       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.30.mlp.gate_proj.weight     0.0805       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.28.mlp.up_proj.weight       0.0714       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.2.self_attn.q_proj.weight   0.0709       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.14.mlp.up_proj.weight       0.0674       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.29.mlp.up_proj.weight       0.0638       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.9.self_attn.k_proj.weight   0.0631       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.12.self_attn.k_proj.weight  0.0619       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.14.self_attn.q_proj.weight  0.0615       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.4.self_attn.k_proj.weight   0.0614       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.17.self_attn.q_proj.weight  0.0611       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.11.self_attn.q_proj.weight  0.0608       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.23.self_attn.q_proj.weight  0.0573       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.9.self_attn.q_proj.weight   0.0570       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.10.mlp.up_proj.weight       0.0562       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.13.self_attn.k_proj.weight  0.0558       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.8.self_attn.q_proj.weight   0.0557       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.7.mlp.gate_proj.weight      0.0554       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.4.mlp.gate_proj.weight      0.0554       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.29.mlp.gate_proj.weight     0.0543       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.8.self_attn.k_proj.weight   0.0533       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.14.self_attn.k_proj.weight  0.0521       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.2.self_attn.k_proj.weight   0.0521       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.1.self_attn.k_proj.weight   0.0504       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.27.self_attn.k_proj.weight  0.0476       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.3.self_attn.q_proj.weight   0.0474       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.15.self_attn.q_proj.weight  0.0472       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.24.self_attn.q_proj.weight  0.0470       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.10.self_attn.q_proj.weight  0.0464       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.11.self_attn.k_proj.weight  0.0462       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.10.self_attn.k_proj.weight  0.0456       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.31.mlp.gate_proj.weight     0.0453       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.29.self_attn.q_proj.weight  0.0453       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.4.self_attn.q_proj.weight   0.0453       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.3.self_attn.k_proj.weight   0.0449       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.5.self_attn.q_proj.weight   0.0441       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.5.self_attn.k_proj.weight   0.0441       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.27.self_attn.q_proj.weight  0.0439       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.23.self_attn.k_proj.weight  0.0439       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.0.mlp.up_proj.weight        0.0436       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.15.self_attn.k_proj.weight  0.0432       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.20.self_attn.q_proj.weight  0.0429       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.21.self_attn.q_proj.weight  0.0428       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.6.self_attn.q_proj.weight   0.0427       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.26.self_attn.q_proj.weight  0.0425       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.7.self_attn.q_proj.weight   0.0423       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.16.self_attn.k_proj.weight  0.0418       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.25.self_attn.q_proj.weight  0.0417       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.18.self_attn.q_proj.weight  0.0416       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.16.self_attn.q_proj.weight  0.0411       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.31.self_attn.q_proj.weight  0.0410       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.0.mlp.gate_proj.weight      0.0402       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.18.self_attn.k_proj.weight  0.0396       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.21.self_attn.k_proj.weight  0.0388       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.7.self_attn.k_proj.weight   0.0386       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.31.self_attn.k_proj.weight  0.0386       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.26.self_attn.k_proj.weight  0.0385       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.20.self_attn.k_proj.weight  0.0377       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.29.self_attn.k_proj.weight  0.0373       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.25.self_attn.k_proj.weight  0.0370       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.17.self_attn.k_proj.weight  0.0370       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.28.self_attn.q_proj.weight  0.0363       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.22.self_attn.q_proj.weight  0.0344       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.17.mlp.up_proj.weight       0.0342       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.19.self_attn.q_proj.weight  0.0336       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.28.self_attn.k_proj.weight  0.0334       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.1.self_attn.q_proj.weight   0.0333       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.24.self_attn.k_proj.weight  0.0323       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.19.self_attn.k_proj.weight  0.0322       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.30.self_attn.k_proj.weight  0.0320       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.30.self_attn.q_proj.weight  0.0308       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.22.self_attn.k_proj.weight  0.0307       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.31.mlp.up_proj.weight       0.0300       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.6.self_attn.k_proj.weight   0.0298       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.0.self_attn.k_proj.weight   0.0293       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "model.layers.30.mlp.up_proj.weight       0.0258       (11008, 4096)   45088768,,,,,,, 1555,,,,,,,,,,, 23486720,,,,,,, 47.91%\n",
      "model.layers.0.self_attn.q_proj.weight   0.0160       (4096, 4096)    16777216,,,,,,, 1555,,,,,,,,,,, 12738560,,,,,,, 24.07%\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Suppress a specific warning from the transformers library for cleaner output.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pad_token_id.*\")\n",
    "\n",
    "def calculate_truncation_ranks(model, importance_dict, compression_ratio, smoothing_alpha):\n",
    "    \"\"\"\n",
    "    Calculates the number of singular values (k) to keep for each layer\n",
    "    based on a compression budget and layer importance scores.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The transformer model to be compressed.\n",
    "        importance_dict (OrderedDict): A dictionary with layer names as keys and\n",
    "                                       their importance scores as values.\n",
    "        compression_ratio (float): The target compression ratio (e.g., 0.6 for 60%).\n",
    "                                   This means the final size should be 40% of the original.\n",
    "        smoothing_alpha (float): A factor to smooth the importance distribution.\n",
    "                                 - 1.0: Ranks are directly proportional to importance.\n",
    "                                 - (0, 1): Differences are smoothed, leading to more\n",
    "                                           uniform ranks. A value closer to 0 means\n",
    "                                           more smoothing.\n",
    "                                 - > 1.0: Differences are exaggerated.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the calculated ranks for each layer.\n",
    "        dict: A dictionary containing detailed stats for each layer.\n",
    "    \"\"\"\n",
    "    print(\"Starting rank calculation...\")\n",
    "    \n",
    "    # --- 1. Collect Layer Information ---\n",
    "    layer_info = {}\n",
    "    total_original_params = 0\n",
    "    \n",
    "    importance_dict = {k + '.weight': v for k, v in importance_dict.items()}\n",
    "\n",
    "    # Filter to only include linear layers present in the importance_dict\n",
    "    compressible_layers = {name: param for name, param in model.named_parameters() if name in importance_dict and isinstance(model.get_submodule(name.rsplit('.', 1)[0]), nn.Linear)}\n",
    "\n",
    "    for name, param in compressible_layers.items():\n",
    "        if param.dim() == 2: # Ensure it's a 2D weight matrix (Linear layer)\n",
    "            rows, cols = param.shape\n",
    "            original_params = rows * cols\n",
    "            \n",
    "            # The number of parameters after SVD truncation to rank k is k * (rows + cols)\n",
    "            param_cost_per_rank = rows + cols\n",
    "            \n",
    "            # Constraint: To ensure compression, k must be less than half the smallest dimension.\n",
    "            # If k >= (rows * cols) / (rows + cols), we are not compressing.\n",
    "            # The user-specified constraint is k < min(rows, cols) / 2.\n",
    "            max_rank = (min(rows, cols) // 2) - 1\n",
    "            \n",
    "            if max_rank <= 0:\n",
    "                print(f\"Skipping layer {name} as it cannot be compressed with the given constraints.\")\n",
    "                continue\n",
    "\n",
    "            layer_info[name] = {\n",
    "                'shape': (rows, cols),\n",
    "                'original_params': original_params,\n",
    "                'cost_per_rank': param_cost_per_rank,\n",
    "                'max_rank': max_rank,\n",
    "                'importance': importance_dict.get(name, 0.0)\n",
    "            }\n",
    "            total_original_params += original_params\n",
    "\n",
    "    if not layer_info:\n",
    "        print(\"No compressible layers found or matched with importance_dict. Aborting.\")\n",
    "        return {}, {}\n",
    "\n",
    "    print(f\"Found {len(layer_info)} compressible layers.\")\n",
    "    print(f\"Total original parameters in these layers: {total_original_params:,}\")\n",
    "\n",
    "    # --- 2. Normalize and Smooth Importance Scores ---\n",
    "    total_importance = sum(info['importance'] for info in layer_info.values())\n",
    "    for name in layer_info:\n",
    "        # Normalize importance to sum to 1\n",
    "        normalized_importance = layer_info[name]['importance'] / total_importance\n",
    "        # Apply smoothing\n",
    "        layer_info[name]['smoothed_importance'] = normalized_importance ** smoothing_alpha\n",
    "\n",
    "    # Renormalize smoothed scores to sum to 1\n",
    "    total_smoothed_importance = sum(info['smoothed_importance'] for info in layer_info.values())\n",
    "    for name in layer_info:\n",
    "        layer_info[name]['final_weight'] = layer_info[name]['smoothed_importance'] / total_smoothed_importance\n",
    "\n",
    "\n",
    "    # --- 3. Iterative Rank Allocation ---\n",
    "    target_total_params = total_original_params * compression_ratio\n",
    "    print(f\"Target parameters after compression: {int(target_total_params):,}\")\n",
    "\n",
    "    # Initialize loop variables\n",
    "    final_ranks = {}\n",
    "    remaining_budget = target_total_params\n",
    "    layers_to_process = list(layer_info.keys())\n",
    "    \n",
    "    is_stable = False\n",
    "    while not is_stable and layers_to_process:\n",
    "        is_stable = True\n",
    "        \n",
    "        # Calculate the allocation constant 'C' based on the current set of layers\n",
    "        # The total budget is sum(k_i * cost_i). We model k_i = C * weight_i.\n",
    "        # So, budget = sum(C * weight_i * cost_i) = C * sum(weight_i * cost_i).\n",
    "        # Therefore, C = budget / sum(weight_i * cost_i).\n",
    "        \n",
    "        current_total_weighted_cost = sum(layer_info[name]['final_weight'] * layer_info[name]['cost_per_rank'] for name in layers_to_process)\n",
    "        \n",
    "        if current_total_weighted_cost == 0:\n",
    "            break # Avoid division by zero if no layers are left\n",
    "\n",
    "        allocation_constant = remaining_budget / current_total_weighted_cost\n",
    "\n",
    "        # Determine tentative ranks and identify layers that exceed their max_rank\n",
    "        newly_capped_layers = []\n",
    "        next_layers_to_process = []\n",
    "\n",
    "        for name in layers_to_process:\n",
    "            info = layer_info[name]\n",
    "            tentative_rank = allocation_constant * info['final_weight']\n",
    "            \n",
    "            if tentative_rank >= info['max_rank']:\n",
    "                # This layer's rank is capped. Fix it and remove from next iteration.\n",
    "                is_stable = False\n",
    "                final_ranks[name] = info['max_rank']\n",
    "                capped_params = info['max_rank'] * info['cost_per_rank']\n",
    "                remaining_budget -= capped_params\n",
    "                newly_capped_layers.append(name)\n",
    "            else:\n",
    "                # This layer is still in contention\n",
    "                next_layers_to_process.append(name)\n",
    "        \n",
    "        layers_to_process = next_layers_to_process\n",
    "\n",
    "    # After the loop, allocate ranks for the remaining (uncapped) layers\n",
    "    if layers_to_process:\n",
    "        current_total_weighted_cost = sum(layer_info[name]['final_weight'] * layer_info[name]['cost_per_rank'] for name in layers_to_process)\n",
    "        if current_total_weighted_cost > 0:\n",
    "            allocation_constant = remaining_budget / current_total_weighted_cost\n",
    "            for name in layers_to_process:\n",
    "                final_ranks[name] = int(max(1, np.floor(allocation_constant * layer_info[name]['final_weight'])))\n",
    "\n",
    "    # --- 4. Final Report Generation ---\n",
    "    detailed_stats = OrderedDict()\n",
    "    total_final_params = 0\n",
    "    for name, info in sorted(layer_info.items(), key=lambda x: x[1]['importance'], reverse=True):\n",
    "        rank = final_ranks.get(name, 0)\n",
    "        new_params = rank * info['cost_per_rank']\n",
    "        total_final_params += new_params\n",
    "        \n",
    "        individual_compression = 1.0 - (new_params / info['original_params']) if info['original_params'] > 0 else 0\n",
    "        \n",
    "        detailed_stats[name] = {\n",
    "            \"shape\": info['shape'],\n",
    "            \"importance\": info['importance'],\n",
    "            \"original_params\": info['original_params'],\n",
    "            \"final_rank_k\": rank,\n",
    "            \"new_params\": new_params,\n",
    "            \"compression\": f\"{individual_compression:.2%}\"\n",
    "        }\n",
    "        \n",
    "    actual_compression_ratio = 1.0 - (total_final_params / total_original_params)\n",
    "\n",
    "    print(\"\\n--- Compression Results ---\")\n",
    "    print(f\"Target Compression Ratio: {compression_ratio:.2%}\")\n",
    "    print(f\"Achieved Compression Ratio: {actual_compression_ratio:.2%}\")\n",
    "    print(f\"Original Parameters: {total_original_params:,}\")\n",
    "    print(f\"Final Parameters: {int(total_final_params):,}\")\n",
    "    print(\"---------------------------\\n\")\n",
    "    \n",
    "    final_ranks = {k.replace('.weight', ''): v for k, v in final_ranks.items()}\n",
    "\n",
    "    return final_ranks, detailed_stats\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    MODEL_ID = \"huggyllama/llama-7b\"\n",
    "    COMPRESSION_RATIO = 0.60  # Target: 60% smaller, 40% of original size\n",
    "    SMOOTHING_ALPHA = 0.0     # Value between 0 and 1. Closer to 0 = more uniform ranks.\n",
    "\n",
    "    # --- 1. Load Model and Importance Scores ---\n",
    "    print(f\"Loading model: {MODEL_ID}. This may take a while...\")\n",
    "    # Using low_cpu_mem_usage to handle large models more efficiently.\n",
    "    # If you have a GPU, you can add device_map='auto'.\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "    \n",
    "    # Load the gradients to verify\n",
    "    with open(\"grads/llama7b_grads_out.pt\", \"rb\") as f:\n",
    "        importance_dict = torch.load(f)\n",
    "        \n",
    "    # Calculate the average importance of each layer\n",
    "    importance_avg = OrderedDict()\n",
    "    for layer_name, importance in importance_dict.items():\n",
    "        importance_avg[layer_name] = torch.mean(importance).item()\n",
    "\n",
    "    # --- 2. Run the Algorithm ---\n",
    "    final_ranks, detailed_stats = calculate_truncation_ranks(\n",
    "        model=model,\n",
    "        importance_dict=importance_avg,\n",
    "        compression_ratio=COMPRESSION_RATIO,\n",
    "        smoothing_alpha=SMOOTHING_ALPHA\n",
    "    )\n",
    "\n",
    "    # --- 3. Print Detailed Layer-by-Layer Results ---\n",
    "    if detailed_stats:\n",
    "        print(f\"{'Layer Name':<40} {'Importance':<12} {'Shape':<15} {'Orig. Params':<15} {'New Rank (k)':<15} {'New Params':<15} {'Compression'}\")\n",
    "        print(\"-\" * 140)\n",
    "        for name, stats in detailed_stats.items():\n",
    "            print(f\"{name:<40} {stats['importance']:<12.4f} {str(stats['shape']):<15} {stats['original_params']:,<15} {stats['final_rank_k']:,<15} {stats['new_params']:,<15} {stats['compression']}\")\n",
    "    \n",
    "    # --- 4. Clean up ---\n",
    "    del model\n",
    "    gc.collect()\n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee21742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228\n"
     ]
    }
   ],
   "source": [
    "def get_truncate(in_features, out_features, ratio):\n",
    "    return int(in_features * out_features * ratio / (in_features + out_features))\n",
    "\n",
    "print(get_truncate(4096, 4096, 0.6))  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee99252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n"
     ]
    }
   ],
   "source": [
    "print(get_truncate(4096, 11008, 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "900fe7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.layers.0.self_attn.q_proj': 1555, 'model.layers.0.self_attn.k_proj': 1555, 'model.layers.0.self_attn.v_proj': 1555, 'model.layers.0.self_attn.o_proj': 1555, 'model.layers.0.mlp.gate_proj': 1555, 'model.layers.0.mlp.up_proj': 1555, 'model.layers.0.mlp.down_proj': 1555, 'model.layers.1.self_attn.q_proj': 1555, 'model.layers.1.self_attn.k_proj': 1555, 'model.layers.1.self_attn.v_proj': 1555, 'model.layers.1.self_attn.o_proj': 1555, 'model.layers.1.mlp.gate_proj': 1555, 'model.layers.1.mlp.up_proj': 1555, 'model.layers.1.mlp.down_proj': 1555, 'model.layers.2.self_attn.q_proj': 1555, 'model.layers.2.self_attn.k_proj': 1555, 'model.layers.2.self_attn.v_proj': 1555, 'model.layers.2.self_attn.o_proj': 1555, 'model.layers.2.mlp.gate_proj': 1555, 'model.layers.2.mlp.up_proj': 1555, 'model.layers.2.mlp.down_proj': 1555, 'model.layers.3.self_attn.q_proj': 1555, 'model.layers.3.self_attn.k_proj': 1555, 'model.layers.3.self_attn.v_proj': 1555, 'model.layers.3.self_attn.o_proj': 1555, 'model.layers.3.mlp.gate_proj': 1555, 'model.layers.3.mlp.up_proj': 1555, 'model.layers.3.mlp.down_proj': 1555, 'model.layers.4.self_attn.q_proj': 1555, 'model.layers.4.self_attn.k_proj': 1555, 'model.layers.4.self_attn.v_proj': 1555, 'model.layers.4.self_attn.o_proj': 1555, 'model.layers.4.mlp.gate_proj': 1555, 'model.layers.4.mlp.up_proj': 1555, 'model.layers.4.mlp.down_proj': 1555, 'model.layers.5.self_attn.q_proj': 1555, 'model.layers.5.self_attn.k_proj': 1555, 'model.layers.5.self_attn.v_proj': 1555, 'model.layers.5.self_attn.o_proj': 1555, 'model.layers.5.mlp.gate_proj': 1555, 'model.layers.5.mlp.up_proj': 1555, 'model.layers.5.mlp.down_proj': 1555, 'model.layers.6.self_attn.q_proj': 1555, 'model.layers.6.self_attn.k_proj': 1555, 'model.layers.6.self_attn.v_proj': 1555, 'model.layers.6.self_attn.o_proj': 1555, 'model.layers.6.mlp.gate_proj': 1555, 'model.layers.6.mlp.up_proj': 1555, 'model.layers.6.mlp.down_proj': 1555, 'model.layers.7.self_attn.q_proj': 1555, 'model.layers.7.self_attn.k_proj': 1555, 'model.layers.7.self_attn.v_proj': 1555, 'model.layers.7.self_attn.o_proj': 1555, 'model.layers.7.mlp.gate_proj': 1555, 'model.layers.7.mlp.up_proj': 1555, 'model.layers.7.mlp.down_proj': 1555, 'model.layers.8.self_attn.q_proj': 1555, 'model.layers.8.self_attn.k_proj': 1555, 'model.layers.8.self_attn.v_proj': 1555, 'model.layers.8.self_attn.o_proj': 1555, 'model.layers.8.mlp.gate_proj': 1555, 'model.layers.8.mlp.up_proj': 1555, 'model.layers.8.mlp.down_proj': 1555, 'model.layers.9.self_attn.q_proj': 1555, 'model.layers.9.self_attn.k_proj': 1555, 'model.layers.9.self_attn.v_proj': 1555, 'model.layers.9.self_attn.o_proj': 1555, 'model.layers.9.mlp.gate_proj': 1555, 'model.layers.9.mlp.up_proj': 1555, 'model.layers.9.mlp.down_proj': 1555, 'model.layers.10.self_attn.q_proj': 1555, 'model.layers.10.self_attn.k_proj': 1555, 'model.layers.10.self_attn.v_proj': 1555, 'model.layers.10.self_attn.o_proj': 1555, 'model.layers.10.mlp.gate_proj': 1555, 'model.layers.10.mlp.up_proj': 1555, 'model.layers.10.mlp.down_proj': 1555, 'model.layers.11.self_attn.q_proj': 1555, 'model.layers.11.self_attn.k_proj': 1555, 'model.layers.11.self_attn.v_proj': 1555, 'model.layers.11.self_attn.o_proj': 1555, 'model.layers.11.mlp.gate_proj': 1555, 'model.layers.11.mlp.up_proj': 1555, 'model.layers.11.mlp.down_proj': 1555, 'model.layers.12.self_attn.q_proj': 1555, 'model.layers.12.self_attn.k_proj': 1555, 'model.layers.12.self_attn.v_proj': 1555, 'model.layers.12.self_attn.o_proj': 1555, 'model.layers.12.mlp.gate_proj': 1555, 'model.layers.12.mlp.up_proj': 1555, 'model.layers.12.mlp.down_proj': 1555, 'model.layers.13.self_attn.q_proj': 1555, 'model.layers.13.self_attn.k_proj': 1555, 'model.layers.13.self_attn.v_proj': 1555, 'model.layers.13.self_attn.o_proj': 1555, 'model.layers.13.mlp.gate_proj': 1555, 'model.layers.13.mlp.up_proj': 1555, 'model.layers.13.mlp.down_proj': 1555, 'model.layers.14.self_attn.q_proj': 1555, 'model.layers.14.self_attn.k_proj': 1555, 'model.layers.14.self_attn.v_proj': 1555, 'model.layers.14.self_attn.o_proj': 1555, 'model.layers.14.mlp.gate_proj': 1555, 'model.layers.14.mlp.up_proj': 1555, 'model.layers.14.mlp.down_proj': 1555, 'model.layers.15.self_attn.q_proj': 1555, 'model.layers.15.self_attn.k_proj': 1555, 'model.layers.15.self_attn.v_proj': 1555, 'model.layers.15.self_attn.o_proj': 1555, 'model.layers.15.mlp.gate_proj': 1555, 'model.layers.15.mlp.up_proj': 1555, 'model.layers.15.mlp.down_proj': 1555, 'model.layers.16.self_attn.q_proj': 1555, 'model.layers.16.self_attn.k_proj': 1555, 'model.layers.16.self_attn.v_proj': 1555, 'model.layers.16.self_attn.o_proj': 1555, 'model.layers.16.mlp.gate_proj': 1555, 'model.layers.16.mlp.up_proj': 1555, 'model.layers.16.mlp.down_proj': 1555, 'model.layers.17.self_attn.q_proj': 1555, 'model.layers.17.self_attn.k_proj': 1555, 'model.layers.17.self_attn.v_proj': 1555, 'model.layers.17.self_attn.o_proj': 1555, 'model.layers.17.mlp.gate_proj': 1555, 'model.layers.17.mlp.up_proj': 1555, 'model.layers.17.mlp.down_proj': 1555, 'model.layers.18.self_attn.q_proj': 1555, 'model.layers.18.self_attn.k_proj': 1555, 'model.layers.18.self_attn.v_proj': 1555, 'model.layers.18.self_attn.o_proj': 1555, 'model.layers.18.mlp.gate_proj': 1555, 'model.layers.18.mlp.up_proj': 1555, 'model.layers.18.mlp.down_proj': 1555, 'model.layers.19.self_attn.q_proj': 1555, 'model.layers.19.self_attn.k_proj': 1555, 'model.layers.19.self_attn.v_proj': 1555, 'model.layers.19.self_attn.o_proj': 1555, 'model.layers.19.mlp.gate_proj': 1555, 'model.layers.19.mlp.up_proj': 1555, 'model.layers.19.mlp.down_proj': 1555, 'model.layers.20.self_attn.q_proj': 1555, 'model.layers.20.self_attn.k_proj': 1555, 'model.layers.20.self_attn.v_proj': 1555, 'model.layers.20.self_attn.o_proj': 1555, 'model.layers.20.mlp.gate_proj': 1555, 'model.layers.20.mlp.up_proj': 1555, 'model.layers.20.mlp.down_proj': 1555, 'model.layers.21.self_attn.q_proj': 1555, 'model.layers.21.self_attn.k_proj': 1555, 'model.layers.21.self_attn.v_proj': 1555, 'model.layers.21.self_attn.o_proj': 1555, 'model.layers.21.mlp.gate_proj': 1555, 'model.layers.21.mlp.up_proj': 1555, 'model.layers.21.mlp.down_proj': 1555, 'model.layers.22.self_attn.q_proj': 1555, 'model.layers.22.self_attn.k_proj': 1555, 'model.layers.22.self_attn.v_proj': 1555, 'model.layers.22.self_attn.o_proj': 1555, 'model.layers.22.mlp.gate_proj': 1555, 'model.layers.22.mlp.up_proj': 1555, 'model.layers.22.mlp.down_proj': 1555, 'model.layers.23.self_attn.q_proj': 1555, 'model.layers.23.self_attn.k_proj': 1555, 'model.layers.23.self_attn.v_proj': 1555, 'model.layers.23.self_attn.o_proj': 1555, 'model.layers.23.mlp.gate_proj': 1555, 'model.layers.23.mlp.up_proj': 1555, 'model.layers.23.mlp.down_proj': 1555, 'model.layers.24.self_attn.q_proj': 1555, 'model.layers.24.self_attn.k_proj': 1555, 'model.layers.24.self_attn.v_proj': 1555, 'model.layers.24.self_attn.o_proj': 1555, 'model.layers.24.mlp.gate_proj': 1555, 'model.layers.24.mlp.up_proj': 1555, 'model.layers.24.mlp.down_proj': 1555, 'model.layers.25.self_attn.q_proj': 1555, 'model.layers.25.self_attn.k_proj': 1555, 'model.layers.25.self_attn.v_proj': 1555, 'model.layers.25.self_attn.o_proj': 1555, 'model.layers.25.mlp.gate_proj': 1555, 'model.layers.25.mlp.up_proj': 1555, 'model.layers.25.mlp.down_proj': 1555, 'model.layers.26.self_attn.q_proj': 1555, 'model.layers.26.self_attn.k_proj': 1555, 'model.layers.26.self_attn.v_proj': 1555, 'model.layers.26.self_attn.o_proj': 1555, 'model.layers.26.mlp.gate_proj': 1555, 'model.layers.26.mlp.up_proj': 1555, 'model.layers.26.mlp.down_proj': 1555, 'model.layers.27.self_attn.q_proj': 1555, 'model.layers.27.self_attn.k_proj': 1555, 'model.layers.27.self_attn.v_proj': 1555, 'model.layers.27.self_attn.o_proj': 1555, 'model.layers.27.mlp.gate_proj': 1555, 'model.layers.27.mlp.up_proj': 1555, 'model.layers.27.mlp.down_proj': 1555, 'model.layers.28.self_attn.q_proj': 1555, 'model.layers.28.self_attn.k_proj': 1555, 'model.layers.28.self_attn.v_proj': 1555, 'model.layers.28.self_attn.o_proj': 1555, 'model.layers.28.mlp.gate_proj': 1555, 'model.layers.28.mlp.up_proj': 1555, 'model.layers.28.mlp.down_proj': 1555, 'model.layers.29.self_attn.q_proj': 1555, 'model.layers.29.self_attn.k_proj': 1555, 'model.layers.29.self_attn.v_proj': 1555, 'model.layers.29.self_attn.o_proj': 1555, 'model.layers.29.mlp.gate_proj': 1555, 'model.layers.29.mlp.up_proj': 1555, 'model.layers.29.mlp.down_proj': 1555, 'model.layers.30.self_attn.q_proj': 1555, 'model.layers.30.self_attn.k_proj': 1555, 'model.layers.30.self_attn.v_proj': 1555, 'model.layers.30.self_attn.o_proj': 1555, 'model.layers.30.mlp.gate_proj': 1555, 'model.layers.30.mlp.up_proj': 1555, 'model.layers.30.mlp.down_proj': 1555, 'model.layers.31.self_attn.q_proj': 1555, 'model.layers.31.self_attn.k_proj': 1555, 'model.layers.31.self_attn.v_proj': 1555, 'model.layers.31.self_attn.o_proj': 1555, 'model.layers.31.mlp.gate_proj': 1555, 'model.layers.31.mlp.up_proj': 1555, 'model.layers.31.mlp.down_proj': 1555}\n"
     ]
    }
   ],
   "source": [
    "print(final_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "884f64ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: huggyllama/llama-7b. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rank calculation with grouped compression...\n",
      "\n",
      "--- Processing Group: self_attn.q_proj (32 layers) ---\n",
      "Group Original Params: 536,870,912\n",
      "Group Target Params:   322,122,547\n",
      "\n",
      "--- Processing Group: self_attn.k_proj (32 layers) ---\n",
      "Group Original Params: 536,870,912\n",
      "Group Target Params:   322,122,547\n",
      "\n",
      "--- Processing Group: self_attn.v_proj (32 layers) ---\n",
      "Group Original Params: 536,870,912\n",
      "Group Target Params:   322,122,547\n",
      "\n",
      "--- Processing Group: self_attn.o_proj (32 layers) ---\n",
      "Group Original Params: 536,870,912\n",
      "Group Target Params:   322,122,547\n",
      "\n",
      "--- Processing Group: mlp.gate_proj (32 layers) ---\n",
      "Group Original Params: 1,442,840,576\n",
      "Group Target Params:   865,704,345\n",
      "\n",
      "--- Processing Group: mlp.up_proj (32 layers) ---\n",
      "Group Original Params: 1,442,840,576\n",
      "Group Target Params:   865,704,345\n",
      "\n",
      "--- Processing Group: mlp.down_proj (32 layers) ---\n",
      "Group Original Params: 1,442,840,576\n",
      "Group Target Params:   865,704,345\n",
      "\n",
      "--- Overall Compression Results ---\n",
      "Target Compression Ratio:   60.00%\n",
      "Achieved Compression Ratio: 40.02%\n",
      "Original Parameters: 6,476,005,376\n",
      "Final Parameters:    3,884,334,592\n",
      "-----------------------------------\n",
      "\n",
      "Group                Layer Name                                    Importance   Shape           Orig. Params    New Rank (k)    New Params      Compression\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mlp.down_proj        model.layers.30.mlp.down_proj.weight          0.4827       (4096, 11008)   45088768,,,,,,, 1898,,,,,,,,,,, 28667392,,,,,,, 36.42%\n",
      "mlp.down_proj        model.layers.29.mlp.down_proj.weight          0.3877       (4096, 11008)   45088768,,,,,,, 1857,,,,,,,,,,, 28048128,,,,,,, 37.79%\n",
      "mlp.down_proj        model.layers.31.mlp.down_proj.weight          0.3867       (4096, 11008)   45088768,,,,,,, 1857,,,,,,,,,,, 28048128,,,,,,, 37.79%\n",
      "mlp.down_proj        model.layers.26.mlp.down_proj.weight          0.3716       (4096, 11008)   45088768,,,,,,, 1849,,,,,,,,,,, 27927296,,,,,,, 38.06%\n",
      "mlp.down_proj        model.layers.27.mlp.down_proj.weight          0.3713       (4096, 11008)   45088768,,,,,,, 1849,,,,,,,,,,, 27927296,,,,,,, 38.06%\n",
      "mlp.down_proj        model.layers.28.mlp.down_proj.weight          0.3667       (4096, 11008)   45088768,,,,,,, 1847,,,,,,,,,,, 27897088,,,,,,, 38.13%\n",
      "mlp.down_proj        model.layers.25.mlp.down_proj.weight          0.3562       (4096, 11008)   45088768,,,,,,, 1842,,,,,,,,,,, 27821568,,,,,,, 38.30%\n",
      "mlp.down_proj        model.layers.24.mlp.down_proj.weight          0.3535       (4096, 11008)   45088768,,,,,,, 1840,,,,,,,,,,, 27791360,,,,,,, 38.36%\n",
      "mlp.down_proj        model.layers.23.mlp.down_proj.weight          0.3499       (4096, 11008)   45088768,,,,,,, 1838,,,,,,,,,,, 27761152,,,,,,, 38.43%\n",
      "mlp.down_proj        model.layers.21.mlp.down_proj.weight          0.2983       (4096, 11008)   45088768,,,,,,, 1809,,,,,,,,,,, 27323136,,,,,,, 39.40%\n",
      "mlp.down_proj        model.layers.22.mlp.down_proj.weight          0.2966       (4096, 11008)   45088768,,,,,,, 1808,,,,,,,,,,, 27308032,,,,,,, 39.43%\n",
      "mlp.down_proj        model.layers.19.mlp.down_proj.weight          0.2947       (4096, 11008)   45088768,,,,,,, 1807,,,,,,,,,,, 27292928,,,,,,, 39.47%\n",
      "mlp.down_proj        model.layers.20.mlp.down_proj.weight          0.2896       (4096, 11008)   45088768,,,,,,, 1804,,,,,,,,,,, 27247616,,,,,,, 39.57%\n",
      "mlp.down_proj        model.layers.18.mlp.down_proj.weight          0.2742       (4096, 11008)   45088768,,,,,,, 1794,,,,,,,,,,, 27096576,,,,,,, 39.90%\n",
      "mlp.down_proj        model.layers.17.mlp.down_proj.weight          0.2634       (4096, 11008)   45088768,,,,,,, 1787,,,,,,,,,,, 26990848,,,,,,, 40.14%\n",
      "mlp.down_proj        model.layers.15.mlp.down_proj.weight          0.2549       (4096, 11008)   45088768,,,,,,, 1781,,,,,,,,,,, 26900224,,,,,,, 40.34%\n",
      "mlp.down_proj        model.layers.16.mlp.down_proj.weight          0.2507       (4096, 11008)   45088768,,,,,,, 1778,,,,,,,,,,, 26854912,,,,,,, 40.44%\n",
      "mlp.down_proj        model.layers.14.mlp.down_proj.weight          0.2499       (4096, 11008)   45088768,,,,,,, 1777,,,,,,,,,,, 26839808,,,,,,, 40.47%\n",
      "mlp.down_proj        model.layers.12.mlp.down_proj.weight          0.2454       (4096, 11008)   45088768,,,,,,, 1774,,,,,,,,,,, 26794496,,,,,,, 40.57%\n",
      "mlp.down_proj        model.layers.13.mlp.down_proj.weight          0.2423       (4096, 11008)   45088768,,,,,,, 1772,,,,,,,,,,, 26764288,,,,,,, 40.64%\n",
      "mlp.down_proj        model.layers.11.mlp.down_proj.weight          0.2383       (4096, 11008)   45088768,,,,,,, 1769,,,,,,,,,,, 26718976,,,,,,, 40.74%\n",
      "mlp.down_proj        model.layers.8.mlp.down_proj.weight           0.2167       (4096, 11008)   45088768,,,,,,, 1752,,,,,,,,,,, 26462208,,,,,,, 41.31%\n",
      "mlp.down_proj        model.layers.7.mlp.down_proj.weight           0.2128       (4096, 11008)   45088768,,,,,,, 1749,,,,,,,,,,, 26416896,,,,,,, 41.41%\n",
      "mlp.down_proj        model.layers.10.mlp.down_proj.weight          0.2100       (4096, 11008)   45088768,,,,,,, 1747,,,,,,,,,,, 26386688,,,,,,, 41.48%\n",
      "mlp.down_proj        model.layers.1.mlp.down_proj.weight           0.2081       (4096, 11008)   45088768,,,,,,, 1745,,,,,,,,,,, 26356480,,,,,,, 41.55%\n",
      "mlp.down_proj        model.layers.4.mlp.down_proj.weight           0.2079       (4096, 11008)   45088768,,,,,,, 1745,,,,,,,,,,, 26356480,,,,,,, 41.55%\n",
      "mlp.down_proj        model.layers.0.mlp.down_proj.weight           0.2076       (4096, 11008)   45088768,,,,,,, 1745,,,,,,,,,,, 26356480,,,,,,, 41.55%\n",
      "mlp.down_proj        model.layers.9.mlp.down_proj.weight           0.2024       (4096, 11008)   45088768,,,,,,, 1740,,,,,,,,,,, 26280960,,,,,,, 41.71%\n",
      "mlp.down_proj        model.layers.2.mlp.down_proj.weight           0.2000       (4096, 11008)   45088768,,,,,,, 1738,,,,,,,,,,, 26250752,,,,,,, 41.78%\n",
      "mlp.down_proj        model.layers.5.mlp.down_proj.weight           0.1973       (4096, 11008)   45088768,,,,,,, 1736,,,,,,,,,,, 26220544,,,,,,, 41.85%\n",
      "mlp.down_proj        model.layers.6.mlp.down_proj.weight           0.1942       (4096, 11008)   45088768,,,,,,, 1733,,,,,,,,,,, 26175232,,,,,,, 41.95%\n",
      "mlp.down_proj        model.layers.3.mlp.down_proj.weight           0.1912       (4096, 11008)   45088768,,,,,,, 1730,,,,,,,,,,, 26129920,,,,,,, 42.05%\n",
      "mlp.gate_proj        model.layers.20.mlp.gate_proj.weight          0.2439       (11008, 4096)   45088768,,,,,,, 1905,,,,,,,,,,, 28773120,,,,,,, 36.19%\n",
      "mlp.gate_proj        model.layers.14.mlp.gate_proj.weight          0.2366       (11008, 4096)   45088768,,,,,,, 1899,,,,,,,,,,, 28682496,,,,,,, 36.39%\n",
      "mlp.gate_proj        model.layers.10.mlp.gate_proj.weight          0.2310       (11008, 4096)   45088768,,,,,,, 1894,,,,,,,,,,, 28606976,,,,,,, 36.55%\n",
      "mlp.gate_proj        model.layers.15.mlp.gate_proj.weight          0.2169       (11008, 4096)   45088768,,,,,,, 1883,,,,,,,,,,, 28440832,,,,,,, 36.92%\n",
      "mlp.gate_proj        model.layers.8.mlp.gate_proj.weight           0.2079       (11008, 4096)   45088768,,,,,,, 1875,,,,,,,,,,, 28320000,,,,,,, 37.19%\n",
      "mlp.gate_proj        model.layers.2.mlp.gate_proj.weight           0.2042       (11008, 4096)   45088768,,,,,,, 1871,,,,,,,,,,, 28259584,,,,,,, 37.32%\n",
      "mlp.gate_proj        model.layers.12.mlp.gate_proj.weight          0.2039       (11008, 4096)   45088768,,,,,,, 1871,,,,,,,,,,, 28259584,,,,,,, 37.32%\n",
      "mlp.gate_proj        model.layers.21.mlp.gate_proj.weight          0.2010       (11008, 4096)   45088768,,,,,,, 1868,,,,,,,,,,, 28214272,,,,,,, 37.43%\n",
      "mlp.gate_proj        model.layers.9.mlp.gate_proj.weight           0.1957       (11008, 4096)   45088768,,,,,,, 1863,,,,,,,,,,, 28138752,,,,,,, 37.59%\n",
      "mlp.gate_proj        model.layers.16.mlp.gate_proj.weight          0.1904       (11008, 4096)   45088768,,,,,,, 1858,,,,,,,,,,, 28063232,,,,,,, 37.76%\n",
      "mlp.gate_proj        model.layers.24.mlp.gate_proj.weight          0.1849       (11008, 4096)   45088768,,,,,,, 1853,,,,,,,,,,, 27987712,,,,,,, 37.93%\n",
      "mlp.gate_proj        model.layers.3.mlp.gate_proj.weight           0.1838       (11008, 4096)   45088768,,,,,,, 1852,,,,,,,,,,, 27972608,,,,,,, 37.96%\n",
      "mlp.gate_proj        model.layers.18.mlp.gate_proj.weight          0.1779       (11008, 4096)   45088768,,,,,,, 1846,,,,,,,,,,, 27881984,,,,,,, 38.16%\n",
      "mlp.gate_proj        model.layers.11.mlp.gate_proj.weight          0.1638       (11008, 4096)   45088768,,,,,,, 1831,,,,,,,,,,, 27655424,,,,,,, 38.66%\n",
      "mlp.gate_proj        model.layers.19.mlp.gate_proj.weight          0.1603       (11008, 4096)   45088768,,,,,,, 1827,,,,,,,,,,, 27595008,,,,,,, 38.80%\n",
      "mlp.gate_proj        model.layers.13.mlp.gate_proj.weight          0.1554       (11008, 4096)   45088768,,,,,,, 1821,,,,,,,,,,, 27504384,,,,,,, 39.00%\n",
      "mlp.gate_proj        model.layers.25.mlp.gate_proj.weight          0.1470       (11008, 4096)   45088768,,,,,,, 1811,,,,,,,,,,, 27353344,,,,,,, 39.33%\n",
      "mlp.gate_proj        model.layers.23.mlp.gate_proj.weight          0.1454       (11008, 4096)   45088768,,,,,,, 1809,,,,,,,,,,, 27323136,,,,,,, 39.40%\n",
      "mlp.gate_proj        model.layers.22.mlp.gate_proj.weight          0.1443       (11008, 4096)   45088768,,,,,,, 1807,,,,,,,,,,, 27292928,,,,,,, 39.47%\n",
      "mlp.gate_proj        model.layers.1.mlp.gate_proj.weight           0.1410       (11008, 4096)   45088768,,,,,,, 1803,,,,,,,,,,, 27232512,,,,,,, 39.60%\n",
      "mlp.gate_proj        model.layers.17.mlp.gate_proj.weight          0.1060       (11008, 4096)   45088768,,,,,,, 1752,,,,,,,,,,, 26462208,,,,,,, 41.31%\n",
      "mlp.gate_proj        model.layers.6.mlp.gate_proj.weight           0.1057       (11008, 4096)   45088768,,,,,,, 1752,,,,,,,,,,, 26462208,,,,,,, 41.31%\n",
      "mlp.gate_proj        model.layers.26.mlp.gate_proj.weight          0.1000       (11008, 4096)   45088768,,,,,,, 1742,,,,,,,,,,, 26311168,,,,,,, 41.65%\n",
      "mlp.gate_proj        model.layers.27.mlp.gate_proj.weight          0.0991       (11008, 4096)   45088768,,,,,,, 1741,,,,,,,,,,, 26296064,,,,,,, 41.68%\n",
      "mlp.gate_proj        model.layers.5.mlp.gate_proj.weight           0.0894       (11008, 4096)   45088768,,,,,,, 1723,,,,,,,,,,, 26024192,,,,,,, 42.28%\n",
      "mlp.gate_proj        model.layers.28.mlp.gate_proj.weight          0.0853       (11008, 4096)   45088768,,,,,,, 1715,,,,,,,,,,, 25903360,,,,,,, 42.55%\n",
      "mlp.gate_proj        model.layers.30.mlp.gate_proj.weight          0.0805       (11008, 4096)   45088768,,,,,,, 1705,,,,,,,,,,, 25752320,,,,,,, 42.89%\n",
      "mlp.gate_proj        model.layers.7.mlp.gate_proj.weight           0.0554       (11008, 4096)   45088768,,,,,,, 1642,,,,,,,,,,, 24800768,,,,,,, 45.00%\n",
      "mlp.gate_proj        model.layers.4.mlp.gate_proj.weight           0.0554       (11008, 4096)   45088768,,,,,,, 1642,,,,,,,,,,, 24800768,,,,,,, 45.00%\n",
      "mlp.gate_proj        model.layers.29.mlp.gate_proj.weight          0.0543       (11008, 4096)   45088768,,,,,,, 1639,,,,,,,,,,, 24755456,,,,,,, 45.10%\n",
      "mlp.gate_proj        model.layers.31.mlp.gate_proj.weight          0.0453       (11008, 4096)   45088768,,,,,,, 1610,,,,,,,,,,, 24317440,,,,,,, 46.07%\n",
      "mlp.gate_proj        model.layers.0.mlp.gate_proj.weight           0.0402       (11008, 4096)   45088768,,,,,,, 1590,,,,,,,,,,, 24015360,,,,,,, 46.74%\n",
      "mlp.up_proj          model.layers.6.mlp.up_proj.weight             0.2725       (11008, 4096)   45088768,,,,,,, 1983,,,,,,,,,,, 29951232,,,,,,, 33.57%\n",
      "mlp.up_proj          model.layers.3.mlp.up_proj.weight             0.2184       (11008, 4096)   45088768,,,,,,, 1939,,,,,,,,,,, 29286656,,,,,,, 35.05%\n",
      "mlp.up_proj          model.layers.2.mlp.up_proj.weight             0.1696       (11008, 4096)   45088768,,,,,,, 1891,,,,,,,,,,, 28561664,,,,,,, 36.65%\n",
      "mlp.up_proj          model.layers.4.mlp.up_proj.weight             0.1577       (11008, 4096)   45088768,,,,,,, 1877,,,,,,,,,,, 28350208,,,,,,, 37.12%\n",
      "mlp.up_proj          model.layers.13.mlp.up_proj.weight            0.1420       (11008, 4096)   45088768,,,,,,, 1858,,,,,,,,,,, 28063232,,,,,,, 37.76%\n",
      "mlp.up_proj          model.layers.5.mlp.up_proj.weight             0.1415       (11008, 4096)   45088768,,,,,,, 1857,,,,,,,,,,, 28048128,,,,,,, 37.79%\n",
      "mlp.up_proj          model.layers.23.mlp.up_proj.weight            0.1324       (11008, 4096)   45088768,,,,,,, 1845,,,,,,,,,,, 27866880,,,,,,, 38.20%\n",
      "mlp.up_proj          model.layers.19.mlp.up_proj.weight            0.1242       (11008, 4096)   45088768,,,,,,, 1833,,,,,,,,,,, 27685632,,,,,,, 38.60%\n",
      "mlp.up_proj          model.layers.11.mlp.up_proj.weight            0.1232       (11008, 4096)   45088768,,,,,,, 1832,,,,,,,,,,, 27670528,,,,,,, 38.63%\n",
      "mlp.up_proj          model.layers.18.mlp.up_proj.weight            0.1228       (11008, 4096)   45088768,,,,,,, 1831,,,,,,,,,,, 27655424,,,,,,, 38.66%\n",
      "mlp.up_proj          model.layers.21.mlp.up_proj.weight            0.1216       (11008, 4096)   45088768,,,,,,, 1829,,,,,,,,,,, 27625216,,,,,,, 38.73%\n",
      "mlp.up_proj          model.layers.20.mlp.up_proj.weight            0.1188       (11008, 4096)   45088768,,,,,,, 1825,,,,,,,,,,, 27564800,,,,,,, 38.87%\n",
      "mlp.up_proj          model.layers.25.mlp.up_proj.weight            0.1182       (11008, 4096)   45088768,,,,,,, 1824,,,,,,,,,,, 27549696,,,,,,, 38.90%\n",
      "mlp.up_proj          model.layers.8.mlp.up_proj.weight             0.1168       (11008, 4096)   45088768,,,,,,, 1822,,,,,,,,,,, 27519488,,,,,,, 38.97%\n",
      "mlp.up_proj          model.layers.7.mlp.up_proj.weight             0.1138       (11008, 4096)   45088768,,,,,,, 1817,,,,,,,,,,, 27443968,,,,,,, 39.13%\n",
      "mlp.up_proj          model.layers.12.mlp.up_proj.weight            0.1133       (11008, 4096)   45088768,,,,,,, 1816,,,,,,,,,,, 27428864,,,,,,, 39.17%\n",
      "mlp.up_proj          model.layers.22.mlp.up_proj.weight            0.1060       (11008, 4096)   45088768,,,,,,, 1804,,,,,,,,,,, 27247616,,,,,,, 39.57%\n",
      "mlp.up_proj          model.layers.15.mlp.up_proj.weight            0.1046       (11008, 4096)   45088768,,,,,,, 1802,,,,,,,,,,, 27217408,,,,,,, 39.64%\n",
      "mlp.up_proj          model.layers.1.mlp.up_proj.weight             0.1041       (11008, 4096)   45088768,,,,,,, 1801,,,,,,,,,,, 27202304,,,,,,, 39.67%\n",
      "mlp.up_proj          model.layers.24.mlp.up_proj.weight            0.1000       (11008, 4096)   45088768,,,,,,, 1794,,,,,,,,,,, 27096576,,,,,,, 39.90%\n",
      "mlp.up_proj          model.layers.27.mlp.up_proj.weight            0.0995       (11008, 4096)   45088768,,,,,,, 1793,,,,,,,,,,, 27081472,,,,,,, 39.94%\n",
      "mlp.up_proj          model.layers.16.mlp.up_proj.weight            0.0969       (11008, 4096)   45088768,,,,,,, 1788,,,,,,,,,,, 27005952,,,,,,, 40.10%\n",
      "mlp.up_proj          model.layers.9.mlp.up_proj.weight             0.0954       (11008, 4096)   45088768,,,,,,, 1785,,,,,,,,,,, 26960640,,,,,,, 40.21%\n",
      "mlp.up_proj          model.layers.26.mlp.up_proj.weight            0.0883       (11008, 4096)   45088768,,,,,,, 1771,,,,,,,,,,, 26749184,,,,,,, 40.67%\n",
      "mlp.up_proj          model.layers.28.mlp.up_proj.weight            0.0714       (11008, 4096)   45088768,,,,,,, 1734,,,,,,,,,,, 26190336,,,,,,, 41.91%\n",
      "mlp.up_proj          model.layers.14.mlp.up_proj.weight            0.0674       (11008, 4096)   45088768,,,,,,, 1724,,,,,,,,,,, 26039296,,,,,,, 42.25%\n",
      "mlp.up_proj          model.layers.29.mlp.up_proj.weight            0.0638       (11008, 4096)   45088768,,,,,,, 1715,,,,,,,,,,, 25903360,,,,,,, 42.55%\n",
      "mlp.up_proj          model.layers.10.mlp.up_proj.weight            0.0562       (11008, 4096)   45088768,,,,,,, 1693,,,,,,,,,,, 25571072,,,,,,, 43.29%\n",
      "mlp.up_proj          model.layers.0.mlp.up_proj.weight             0.0436       (11008, 4096)   45088768,,,,,,, 1651,,,,,,,,,,, 24936704,,,,,,, 44.69%\n",
      "mlp.up_proj          model.layers.17.mlp.up_proj.weight            0.0342       (11008, 4096)   45088768,,,,,,, 1611,,,,,,,,,,, 24332544,,,,,,, 46.03%\n",
      "mlp.up_proj          model.layers.31.mlp.up_proj.weight            0.0300       (11008, 4096)   45088768,,,,,,, 1590,,,,,,,,,,, 24015360,,,,,,, 46.74%\n",
      "mlp.up_proj          model.layers.30.mlp.up_proj.weight            0.0258       (11008, 4096)   45088768,,,,,,, 1566,,,,,,,,,,, 23652864,,,,,,, 47.54%\n",
      "self_attn.k_proj     model.layers.9.self_attn.k_proj.weight        0.0631       (4096, 4096)    16777216,,,,,,, 1280,,,,,,,,,,, 10485760,,,,,,, 37.50%\n",
      "self_attn.k_proj     model.layers.12.self_attn.k_proj.weight       0.0619       (4096, 4096)    16777216,,,,,,, 1277,,,,,,,,,,, 10461184,,,,,,, 37.65%\n",
      "self_attn.k_proj     model.layers.4.self_attn.k_proj.weight        0.0614       (4096, 4096)    16777216,,,,,,, 1276,,,,,,,,,,, 10452992,,,,,,, 37.70%\n",
      "self_attn.k_proj     model.layers.13.self_attn.k_proj.weight       0.0558       (4096, 4096)    16777216,,,,,,, 1264,,,,,,,,,,, 10354688,,,,,,, 38.28%\n",
      "self_attn.k_proj     model.layers.8.self_attn.k_proj.weight        0.0533       (4096, 4096)    16777216,,,,,,, 1258,,,,,,,,,,, 10305536,,,,,,, 38.57%\n",
      "self_attn.k_proj     model.layers.14.self_attn.k_proj.weight       0.0521       (4096, 4096)    16777216,,,,,,, 1255,,,,,,,,,,, 10280960,,,,,,, 38.72%\n",
      "self_attn.k_proj     model.layers.2.self_attn.k_proj.weight        0.0521       (4096, 4096)    16777216,,,,,,, 1255,,,,,,,,,,, 10280960,,,,,,, 38.72%\n",
      "self_attn.k_proj     model.layers.1.self_attn.k_proj.weight        0.0504       (4096, 4096)    16777216,,,,,,, 1251,,,,,,,,,,, 10248192,,,,,,, 38.92%\n",
      "self_attn.k_proj     model.layers.27.self_attn.k_proj.weight       0.0476       (4096, 4096)    16777216,,,,,,, 1244,,,,,,,,,,, 10190848,,,,,,, 39.26%\n",
      "self_attn.k_proj     model.layers.11.self_attn.k_proj.weight       0.0462       (4096, 4096)    16777216,,,,,,, 1240,,,,,,,,,,, 10158080,,,,,,, 39.45%\n",
      "self_attn.k_proj     model.layers.10.self_attn.k_proj.weight       0.0456       (4096, 4096)    16777216,,,,,,, 1239,,,,,,,,,,, 10149888,,,,,,, 39.50%\n",
      "self_attn.k_proj     model.layers.3.self_attn.k_proj.weight        0.0449       (4096, 4096)    16777216,,,,,,, 1237,,,,,,,,,,, 10133504,,,,,,, 39.60%\n",
      "self_attn.k_proj     model.layers.5.self_attn.k_proj.weight        0.0441       (4096, 4096)    16777216,,,,,,, 1234,,,,,,,,,,, 10108928,,,,,,, 39.75%\n",
      "self_attn.k_proj     model.layers.23.self_attn.k_proj.weight       0.0439       (4096, 4096)    16777216,,,,,,, 1234,,,,,,,,,,, 10108928,,,,,,, 39.75%\n",
      "self_attn.k_proj     model.layers.15.self_attn.k_proj.weight       0.0432       (4096, 4096)    16777216,,,,,,, 1232,,,,,,,,,,, 10092544,,,,,,, 39.84%\n",
      "self_attn.k_proj     model.layers.16.self_attn.k_proj.weight       0.0418       (4096, 4096)    16777216,,,,,,, 1228,,,,,,,,,,, 10059776,,,,,,, 40.04%\n",
      "self_attn.k_proj     model.layers.18.self_attn.k_proj.weight       0.0396       (4096, 4096)    16777216,,,,,,, 1221,,,,,,,,,,, 10002432,,,,,,, 40.38%\n",
      "self_attn.k_proj     model.layers.21.self_attn.k_proj.weight       0.0388       (4096, 4096)    16777216,,,,,,, 1219,,,,,,,,,,, 9986048,,,,,,,, 40.48%\n",
      "self_attn.k_proj     model.layers.7.self_attn.k_proj.weight        0.0386       (4096, 4096)    16777216,,,,,,, 1218,,,,,,,,,,, 9977856,,,,,,,, 40.53%\n",
      "self_attn.k_proj     model.layers.31.self_attn.k_proj.weight       0.0386       (4096, 4096)    16777216,,,,,,, 1218,,,,,,,,,,, 9977856,,,,,,,, 40.53%\n",
      "self_attn.k_proj     model.layers.26.self_attn.k_proj.weight       0.0385       (4096, 4096)    16777216,,,,,,, 1218,,,,,,,,,,, 9977856,,,,,,,, 40.53%\n",
      "self_attn.k_proj     model.layers.20.self_attn.k_proj.weight       0.0377       (4096, 4096)    16777216,,,,,,, 1215,,,,,,,,,,, 9953280,,,,,,,, 40.67%\n",
      "self_attn.k_proj     model.layers.29.self_attn.k_proj.weight       0.0373       (4096, 4096)    16777216,,,,,,, 1214,,,,,,,,,,, 9945088,,,,,,,, 40.72%\n",
      "self_attn.k_proj     model.layers.25.self_attn.k_proj.weight       0.0370       (4096, 4096)    16777216,,,,,,, 1213,,,,,,,,,,, 9936896,,,,,,,, 40.77%\n",
      "self_attn.k_proj     model.layers.17.self_attn.k_proj.weight       0.0370       (4096, 4096)    16777216,,,,,,, 1213,,,,,,,,,,, 9936896,,,,,,,, 40.77%\n",
      "self_attn.k_proj     model.layers.28.self_attn.k_proj.weight       0.0334       (4096, 4096)    16777216,,,,,,, 1201,,,,,,,,,,, 9838592,,,,,,,, 41.36%\n",
      "self_attn.k_proj     model.layers.24.self_attn.k_proj.weight       0.0323       (4096, 4096)    16777216,,,,,,, 1197,,,,,,,,,,, 9805824,,,,,,,, 41.55%\n",
      "self_attn.k_proj     model.layers.19.self_attn.k_proj.weight       0.0322       (4096, 4096)    16777216,,,,,,, 1196,,,,,,,,,,, 9797632,,,,,,,, 41.60%\n",
      "self_attn.k_proj     model.layers.30.self_attn.k_proj.weight       0.0320       (4096, 4096)    16777216,,,,,,, 1196,,,,,,,,,,, 9797632,,,,,,,, 41.60%\n",
      "self_attn.k_proj     model.layers.22.self_attn.k_proj.weight       0.0307       (4096, 4096)    16777216,,,,,,, 1191,,,,,,,,,,, 9756672,,,,,,,, 41.85%\n",
      "self_attn.k_proj     model.layers.6.self_attn.k_proj.weight        0.0298       (4096, 4096)    16777216,,,,,,, 1187,,,,,,,,,,, 9723904,,,,,,,, 42.04%\n",
      "self_attn.k_proj     model.layers.0.self_attn.k_proj.weight        0.0293       (4096, 4096)    16777216,,,,,,, 1185,,,,,,,,,,, 9707520,,,,,,,, 42.14%\n",
      "self_attn.o_proj     model.layers.31.self_attn.o_proj.weight       0.6777       (4096, 4096)    16777216,,,,,,, 1344,,,,,,,,,,, 11010048,,,,,,, 34.38%\n",
      "self_attn.o_proj     model.layers.26.self_attn.o_proj.weight       0.3892       (4096, 4096)    16777216,,,,,,, 1271,,,,,,,,,,, 10412032,,,,,,, 37.94%\n",
      "self_attn.o_proj     model.layers.30.self_attn.o_proj.weight       0.3828       (4096, 4096)    16777216,,,,,,, 1269,,,,,,,,,,, 10395648,,,,,,, 38.04%\n",
      "self_attn.o_proj     model.layers.27.self_attn.o_proj.weight       0.3796       (4096, 4096)    16777216,,,,,,, 1268,,,,,,,,,,, 10387456,,,,,,, 38.09%\n",
      "self_attn.o_proj     model.layers.25.self_attn.o_proj.weight       0.3726       (4096, 4096)    16777216,,,,,,, 1266,,,,,,,,,,, 10371072,,,,,,, 38.18%\n",
      "self_attn.o_proj     model.layers.28.self_attn.o_proj.weight       0.3726       (4096, 4096)    16777216,,,,,,, 1266,,,,,,,,,,, 10371072,,,,,,, 38.18%\n",
      "self_attn.o_proj     model.layers.23.self_attn.o_proj.weight       0.3694       (4096, 4096)    16777216,,,,,,, 1264,,,,,,,,,,, 10354688,,,,,,, 38.28%\n",
      "self_attn.o_proj     model.layers.24.self_attn.o_proj.weight       0.3667       (4096, 4096)    16777216,,,,,,, 1264,,,,,,,,,,, 10354688,,,,,,, 38.28%\n",
      "self_attn.o_proj     model.layers.29.self_attn.o_proj.weight       0.3640       (4096, 4096)    16777216,,,,,,, 1263,,,,,,,,,,, 10346496,,,,,,, 38.33%\n",
      "self_attn.o_proj     model.layers.22.self_attn.o_proj.weight       0.3164       (4096, 4096)    16777216,,,,,,, 1245,,,,,,,,,,, 10199040,,,,,,, 39.21%\n",
      "self_attn.o_proj     model.layers.21.self_attn.o_proj.weight       0.3162       (4096, 4096)    16777216,,,,,,, 1245,,,,,,,,,,, 10199040,,,,,,, 39.21%\n",
      "self_attn.o_proj     model.layers.19.self_attn.o_proj.weight       0.3115       (4096, 4096)    16777216,,,,,,, 1243,,,,,,,,,,, 10182656,,,,,,, 39.31%\n",
      "self_attn.o_proj     model.layers.20.self_attn.o_proj.weight       0.3083       (4096, 4096)    16777216,,,,,,, 1242,,,,,,,,,,, 10174464,,,,,,, 39.36%\n",
      "self_attn.o_proj     model.layers.18.self_attn.o_proj.weight       0.2932       (4096, 4096)    16777216,,,,,,, 1236,,,,,,,,,,, 10125312,,,,,,, 39.65%\n",
      "self_attn.o_proj     model.layers.16.self_attn.o_proj.weight       0.2708       (4096, 4096)    16777216,,,,,,, 1226,,,,,,,,,,, 10043392,,,,,,, 40.14%\n",
      "self_attn.o_proj     model.layers.17.self_attn.o_proj.weight       0.2681       (4096, 4096)    16777216,,,,,,, 1225,,,,,,,,,,, 10035200,,,,,,, 40.19%\n",
      "self_attn.o_proj     model.layers.15.self_attn.o_proj.weight       0.2603       (4096, 4096)    16777216,,,,,,, 1221,,,,,,,,,,, 10002432,,,,,,, 40.38%\n",
      "self_attn.o_proj     model.layers.13.self_attn.o_proj.weight       0.2483       (4096, 4096)    16777216,,,,,,, 1215,,,,,,,,,,, 9953280,,,,,,,, 40.67%\n",
      "self_attn.o_proj     model.layers.12.self_attn.o_proj.weight       0.2469       (4096, 4096)    16777216,,,,,,, 1215,,,,,,,,,,, 9953280,,,,,,,, 40.67%\n",
      "self_attn.o_proj     model.layers.14.self_attn.o_proj.weight       0.2435       (4096, 4096)    16777216,,,,,,, 1213,,,,,,,,,,, 9936896,,,,,,,, 40.77%\n",
      "self_attn.o_proj     model.layers.11.self_attn.o_proj.weight       0.2413       (4096, 4096)    16777216,,,,,,, 1212,,,,,,,,,,, 9928704,,,,,,,, 40.82%\n",
      "self_attn.o_proj     model.layers.8.self_attn.o_proj.weight        0.2255       (4096, 4096)    16777216,,,,,,, 1203,,,,,,,,,,, 9854976,,,,,,,, 41.26%\n",
      "self_attn.o_proj     model.layers.2.self_attn.o_proj.weight        0.2246       (4096, 4096)    16777216,,,,,,, 1203,,,,,,,,,,, 9854976,,,,,,,, 41.26%\n",
      "self_attn.o_proj     model.layers.4.self_attn.o_proj.weight        0.2181       (4096, 4096)    16777216,,,,,,, 1200,,,,,,,,,,, 9830400,,,,,,,, 41.41%\n",
      "self_attn.o_proj     model.layers.5.self_attn.o_proj.weight        0.2162       (4096, 4096)    16777216,,,,,,, 1198,,,,,,,,,,, 9814016,,,,,,,, 41.50%\n",
      "self_attn.o_proj     model.layers.3.self_attn.o_proj.weight        0.2151       (4096, 4096)    16777216,,,,,,, 1198,,,,,,,,,,, 9814016,,,,,,,, 41.50%\n",
      "self_attn.o_proj     model.layers.7.self_attn.o_proj.weight        0.2128       (4096, 4096)    16777216,,,,,,, 1197,,,,,,,,,,, 9805824,,,,,,,, 41.55%\n",
      "self_attn.o_proj     model.layers.1.self_attn.o_proj.weight        0.2118       (4096, 4096)    16777216,,,,,,, 1196,,,,,,,,,,, 9797632,,,,,,,, 41.60%\n",
      "self_attn.o_proj     model.layers.9.self_attn.o_proj.weight        0.2113       (4096, 4096)    16777216,,,,,,, 1196,,,,,,,,,,, 9797632,,,,,,,, 41.60%\n",
      "self_attn.o_proj     model.layers.6.self_attn.o_proj.weight        0.2061       (4096, 4096)    16777216,,,,,,, 1193,,,,,,,,,,, 9773056,,,,,,,, 41.75%\n",
      "self_attn.o_proj     model.layers.10.self_attn.o_proj.weight       0.2031       (4096, 4096)    16777216,,,,,,, 1191,,,,,,,,,,, 9756672,,,,,,,, 41.85%\n",
      "self_attn.o_proj     model.layers.0.self_attn.o_proj.weight        0.1127       (4096, 4096)    16777216,,,,,,, 1123,,,,,,,,,,, 9199616,,,,,,,, 45.17%\n",
      "self_attn.q_proj     model.layers.12.self_attn.q_proj.weight       0.0841       (4096, 4096)    16777216,,,,,,, 1306,,,,,,,,,,, 10698752,,,,,,, 36.23%\n",
      "self_attn.q_proj     model.layers.13.self_attn.q_proj.weight       0.0813       (4096, 4096)    16777216,,,,,,, 1301,,,,,,,,,,, 10657792,,,,,,, 36.47%\n",
      "self_attn.q_proj     model.layers.2.self_attn.q_proj.weight        0.0709       (4096, 4096)    16777216,,,,,,, 1283,,,,,,,,,,, 10510336,,,,,,, 37.35%\n",
      "self_attn.q_proj     model.layers.14.self_attn.q_proj.weight       0.0615       (4096, 4096)    16777216,,,,,,, 1265,,,,,,,,,,, 10362880,,,,,,, 38.23%\n",
      "self_attn.q_proj     model.layers.17.self_attn.q_proj.weight       0.0611       (4096, 4096)    16777216,,,,,,, 1265,,,,,,,,,,, 10362880,,,,,,, 38.23%\n",
      "self_attn.q_proj     model.layers.11.self_attn.q_proj.weight       0.0608       (4096, 4096)    16777216,,,,,,, 1264,,,,,,,,,,, 10354688,,,,,,, 38.28%\n",
      "self_attn.q_proj     model.layers.23.self_attn.q_proj.weight       0.0573       (4096, 4096)    16777216,,,,,,, 1257,,,,,,,,,,, 10297344,,,,,,, 38.62%\n",
      "self_attn.q_proj     model.layers.9.self_attn.q_proj.weight        0.0570       (4096, 4096)    16777216,,,,,,, 1256,,,,,,,,,,, 10289152,,,,,,, 38.67%\n",
      "self_attn.q_proj     model.layers.8.self_attn.q_proj.weight        0.0557       (4096, 4096)    16777216,,,,,,, 1253,,,,,,,,,,, 10264576,,,,,,, 38.82%\n",
      "self_attn.q_proj     model.layers.3.self_attn.q_proj.weight        0.0474       (4096, 4096)    16777216,,,,,,, 1233,,,,,,,,,,, 10100736,,,,,,, 39.79%\n",
      "self_attn.q_proj     model.layers.15.self_attn.q_proj.weight       0.0472       (4096, 4096)    16777216,,,,,,, 1232,,,,,,,,,,, 10092544,,,,,,, 39.84%\n",
      "self_attn.q_proj     model.layers.24.self_attn.q_proj.weight       0.0470       (4096, 4096)    16777216,,,,,,, 1232,,,,,,,,,,, 10092544,,,,,,, 39.84%\n",
      "self_attn.q_proj     model.layers.10.self_attn.q_proj.weight       0.0464       (4096, 4096)    16777216,,,,,,, 1230,,,,,,,,,,, 10076160,,,,,,, 39.94%\n",
      "self_attn.q_proj     model.layers.29.self_attn.q_proj.weight       0.0453       (4096, 4096)    16777216,,,,,,, 1227,,,,,,,,,,, 10051584,,,,,,, 40.09%\n",
      "self_attn.q_proj     model.layers.4.self_attn.q_proj.weight        0.0453       (4096, 4096)    16777216,,,,,,, 1227,,,,,,,,,,, 10051584,,,,,,, 40.09%\n",
      "self_attn.q_proj     model.layers.5.self_attn.q_proj.weight        0.0441       (4096, 4096)    16777216,,,,,,, 1224,,,,,,,,,,, 10027008,,,,,,, 40.23%\n",
      "self_attn.q_proj     model.layers.27.self_attn.q_proj.weight       0.0439       (4096, 4096)    16777216,,,,,,, 1224,,,,,,,,,,, 10027008,,,,,,, 40.23%\n",
      "self_attn.q_proj     model.layers.20.self_attn.q_proj.weight       0.0429       (4096, 4096)    16777216,,,,,,, 1221,,,,,,,,,,, 10002432,,,,,,, 40.38%\n",
      "self_attn.q_proj     model.layers.21.self_attn.q_proj.weight       0.0428       (4096, 4096)    16777216,,,,,,, 1220,,,,,,,,,,, 9994240,,,,,,,, 40.43%\n",
      "self_attn.q_proj     model.layers.6.self_attn.q_proj.weight        0.0427       (4096, 4096)    16777216,,,,,,, 1220,,,,,,,,,,, 9994240,,,,,,,, 40.43%\n",
      "self_attn.q_proj     model.layers.26.self_attn.q_proj.weight       0.0425       (4096, 4096)    16777216,,,,,,, 1220,,,,,,,,,,, 9994240,,,,,,,, 40.43%\n",
      "self_attn.q_proj     model.layers.7.self_attn.q_proj.weight        0.0423       (4096, 4096)    16777216,,,,,,, 1219,,,,,,,,,,, 9986048,,,,,,,, 40.48%\n",
      "self_attn.q_proj     model.layers.25.self_attn.q_proj.weight       0.0417       (4096, 4096)    16777216,,,,,,, 1217,,,,,,,,,,, 9969664,,,,,,,, 40.58%\n",
      "self_attn.q_proj     model.layers.18.self_attn.q_proj.weight       0.0416       (4096, 4096)    16777216,,,,,,, 1217,,,,,,,,,,, 9969664,,,,,,,, 40.58%\n",
      "self_attn.q_proj     model.layers.16.self_attn.q_proj.weight       0.0411       (4096, 4096)    16777216,,,,,,, 1215,,,,,,,,,,, 9953280,,,,,,,, 40.67%\n",
      "self_attn.q_proj     model.layers.31.self_attn.q_proj.weight       0.0410       (4096, 4096)    16777216,,,,,,, 1215,,,,,,,,,,, 9953280,,,,,,,, 40.67%\n",
      "self_attn.q_proj     model.layers.28.self_attn.q_proj.weight       0.0363       (4096, 4096)    16777216,,,,,,, 1200,,,,,,,,,,, 9830400,,,,,,,, 41.41%\n",
      "self_attn.q_proj     model.layers.22.self_attn.q_proj.weight       0.0344       (4096, 4096)    16777216,,,,,,, 1194,,,,,,,,,,, 9781248,,,,,,,, 41.70%\n",
      "self_attn.q_proj     model.layers.19.self_attn.q_proj.weight       0.0336       (4096, 4096)    16777216,,,,,,, 1191,,,,,,,,,,, 9756672,,,,,,,, 41.85%\n",
      "self_attn.q_proj     model.layers.1.self_attn.q_proj.weight        0.0333       (4096, 4096)    16777216,,,,,,, 1190,,,,,,,,,,, 9748480,,,,,,,, 41.89%\n",
      "self_attn.q_proj     model.layers.30.self_attn.q_proj.weight       0.0308       (4096, 4096)    16777216,,,,,,, 1181,,,,,,,,,,, 9674752,,,,,,,, 42.33%\n",
      "self_attn.q_proj     model.layers.0.self_attn.q_proj.weight        0.0160       (4096, 4096)    16777216,,,,,,, 1106,,,,,,,,,,, 9060352,,,,,,,, 46.00%\n",
      "self_attn.v_proj     model.layers.12.self_attn.v_proj.weight       0.4106       (4096, 4096)    16777216,,,,,,, 1281,,,,,,,,,,, 10493952,,,,,,, 37.45%\n",
      "self_attn.v_proj     model.layers.14.self_attn.v_proj.weight       0.3994       (4096, 4096)    16777216,,,,,,, 1278,,,,,,,,,,, 10469376,,,,,,, 37.60%\n",
      "self_attn.v_proj     model.layers.25.self_attn.v_proj.weight       0.3965       (4096, 4096)    16777216,,,,,,, 1277,,,,,,,,,,, 10461184,,,,,,, 37.65%\n",
      "self_attn.v_proj     model.layers.5.self_attn.v_proj.weight        0.3801       (4096, 4096)    16777216,,,,,,, 1272,,,,,,,,,,, 10420224,,,,,,, 37.89%\n",
      "self_attn.v_proj     model.layers.29.self_attn.v_proj.weight       0.3787       (4096, 4096)    16777216,,,,,,, 1271,,,,,,,,,,, 10412032,,,,,,, 37.94%\n",
      "self_attn.v_proj     model.layers.13.self_attn.v_proj.weight       0.3779       (4096, 4096)    16777216,,,,,,, 1271,,,,,,,,,,, 10412032,,,,,,, 37.94%\n",
      "self_attn.v_proj     model.layers.3.self_attn.v_proj.weight        0.3708       (4096, 4096)    16777216,,,,,,, 1268,,,,,,,,,,, 10387456,,,,,,, 38.09%\n",
      "self_attn.v_proj     model.layers.17.self_attn.v_proj.weight       0.3647       (4096, 4096)    16777216,,,,,,, 1266,,,,,,,,,,, 10371072,,,,,,, 38.18%\n",
      "self_attn.v_proj     model.layers.23.self_attn.v_proj.weight       0.3621       (4096, 4096)    16777216,,,,,,, 1265,,,,,,,,,,, 10362880,,,,,,, 38.23%\n",
      "self_attn.v_proj     model.layers.18.self_attn.v_proj.weight       0.3608       (4096, 4096)    16777216,,,,,,, 1265,,,,,,,,,,, 10362880,,,,,,, 38.23%\n",
      "self_attn.v_proj     model.layers.26.self_attn.v_proj.weight       0.3594       (4096, 4096)    16777216,,,,,,, 1264,,,,,,,,,,, 10354688,,,,,,, 38.28%\n",
      "self_attn.v_proj     model.layers.15.self_attn.v_proj.weight       0.3481       (4096, 4096)    16777216,,,,,,, 1260,,,,,,,,,,, 10321920,,,,,,, 38.48%\n",
      "self_attn.v_proj     model.layers.9.self_attn.v_proj.weight        0.3411       (4096, 4096)    16777216,,,,,,, 1258,,,,,,,,,,, 10305536,,,,,,, 38.57%\n",
      "self_attn.v_proj     model.layers.28.self_attn.v_proj.weight       0.3291       (4096, 4096)    16777216,,,,,,, 1253,,,,,,,,,,, 10264576,,,,,,, 38.82%\n",
      "self_attn.v_proj     model.layers.10.self_attn.v_proj.weight       0.3220       (4096, 4096)    16777216,,,,,,, 1251,,,,,,,,,,, 10248192,,,,,,, 38.92%\n",
      "self_attn.v_proj     model.layers.21.self_attn.v_proj.weight       0.3206       (4096, 4096)    16777216,,,,,,, 1250,,,,,,,,,,, 10240000,,,,,,, 38.96%\n",
      "self_attn.v_proj     model.layers.16.self_attn.v_proj.weight       0.3062       (4096, 4096)    16777216,,,,,,, 1244,,,,,,,,,,, 10190848,,,,,,, 39.26%\n",
      "self_attn.v_proj     model.layers.2.self_attn.v_proj.weight        0.2988       (4096, 4096)    16777216,,,,,,, 1241,,,,,,,,,,, 10166272,,,,,,, 39.40%\n",
      "self_attn.v_proj     model.layers.27.self_attn.v_proj.weight       0.2969       (4096, 4096)    16777216,,,,,,, 1240,,,,,,,,,,, 10158080,,,,,,, 39.45%\n",
      "self_attn.v_proj     model.layers.7.self_attn.v_proj.weight        0.2961       (4096, 4096)    16777216,,,,,,, 1240,,,,,,,,,,, 10158080,,,,,,, 39.45%\n",
      "self_attn.v_proj     model.layers.20.self_attn.v_proj.weight       0.2932       (4096, 4096)    16777216,,,,,,, 1239,,,,,,,,,,, 10149888,,,,,,, 39.50%\n",
      "self_attn.v_proj     model.layers.24.self_attn.v_proj.weight       0.2622       (4096, 4096)    16777216,,,,,,, 1225,,,,,,,,,,, 10035200,,,,,,, 40.19%\n",
      "self_attn.v_proj     model.layers.19.self_attn.v_proj.weight       0.2607       (4096, 4096)    16777216,,,,,,, 1224,,,,,,,,,,, 10027008,,,,,,, 40.23%\n",
      "self_attn.v_proj     model.layers.11.self_attn.v_proj.weight       0.2120       (4096, 4096)    16777216,,,,,,, 1199,,,,,,,,,,, 9822208,,,,,,,, 41.46%\n",
      "self_attn.v_proj     model.layers.22.self_attn.v_proj.weight       0.2017       (4096, 4096)    16777216,,,,,,, 1193,,,,,,,,,,, 9773056,,,,,,,, 41.75%\n",
      "self_attn.v_proj     model.layers.30.self_attn.v_proj.weight       0.2002       (4096, 4096)    16777216,,,,,,, 1193,,,,,,,,,,, 9773056,,,,,,,, 41.75%\n",
      "self_attn.v_proj     model.layers.6.self_attn.v_proj.weight        0.1448       (4096, 4096)    16777216,,,,,,, 1155,,,,,,,,,,, 9461760,,,,,,,, 43.60%\n",
      "self_attn.v_proj     model.layers.31.self_attn.v_proj.weight       0.1425       (4096, 4096)    16777216,,,,,,, 1153,,,,,,,,,,, 9445376,,,,,,,, 43.70%\n",
      "self_attn.v_proj     model.layers.1.self_attn.v_proj.weight        0.1339       (4096, 4096)    16777216,,,,,,, 1146,,,,,,,,,,, 9388032,,,,,,,, 44.04%\n",
      "self_attn.v_proj     model.layers.0.self_attn.v_proj.weight        0.1096       (4096, 4096)    16777216,,,,,,, 1123,,,,,,,,,,, 9199616,,,,,,,, 45.17%\n",
      "self_attn.v_proj     model.layers.4.self_attn.v_proj.weight        0.1083       (4096, 4096)    16777216,,,,,,, 1121,,,,,,,,,,, 9183232,,,,,,,, 45.26%\n",
      "self_attn.v_proj     model.layers.8.self_attn.v_proj.weight        0.1041       (4096, 4096)    16777216,,,,,,, 1117,,,,,,,,,,, 9150464,,,,,,,, 45.46%\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from collections import OrderedDict, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Suppress a specific warning from the transformers library for cleaner output.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pad_token_id.*\")\n",
    "\n",
    "def calculate_truncation_ranks(model, importance_dict, compression_ratio, smoothing_alpha):\n",
    "    \"\"\"\n",
    "    Calculates the number of singular values (k) to keep for each layer,\n",
    "    applying the compression budget independently to each group of layers (e.g.,\n",
    "    all 'q_proj' layers, all 'down_proj' layers, etc.).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The transformer model to be compressed.\n",
    "        importance_dict (OrderedDict): A dictionary with layer names as keys and\n",
    "                                       their importance scores as values.\n",
    "        compression_ratio (float): The target compression ratio (e.g., 0.6 for 60%).\n",
    "                                   This means the final size should be 40% of the original.\n",
    "        smoothing_alpha (float): A factor to smooth the importance distribution.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the calculated ranks for each layer (with clean keys).\n",
    "        dict: A dictionary containing detailed stats for each layer.\n",
    "    \"\"\"\n",
    "    print(\"Starting rank calculation with grouped compression...\")\n",
    "\n",
    "    # --- 1. Group Layers by Type ---\n",
    "    grouped_layer_info = defaultdict(dict)\n",
    "    # Regex to extract the layer type (e.g., 'self_attn.q_proj') from the full name\n",
    "    layer_type_re = re.compile(r'.*\\.layers\\.\\d+\\.([a-zA-Z_]+\\.[a-zA-Z_]+proj)')\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        module_name = name.rsplit('.', 1)[0]\n",
    "        is_in_importance_dict = name in importance_dict or module_name in importance_dict\n",
    "        \n",
    "        try:\n",
    "            is_linear_layer = isinstance(model.get_submodule(module_name), nn.Linear)\n",
    "        except AttributeError:\n",
    "            is_linear_layer = False\n",
    "\n",
    "        if is_linear_layer and is_in_importance_dict and param.dim() == 2:\n",
    "            match = layer_type_re.match(name)\n",
    "            if not match:\n",
    "                print(f\"Warning: Could not determine group for layer {name}. Skipping.\")\n",
    "                continue\n",
    "            group_key = match.group(1)\n",
    "\n",
    "            importance_key = name if name in importance_dict else module_name\n",
    "            importance_score = importance_dict.get(importance_key, 0.0)\n",
    "            if isinstance(importance_score, torch.Tensor):\n",
    "                importance_score = torch.mean(torch.abs(importance_score)).item()\n",
    "\n",
    "            rows, cols = param.shape\n",
    "            max_rank = (min(rows, cols) // 2) - 1\n",
    "            if max_rank <= 0:\n",
    "                continue\n",
    "\n",
    "            grouped_layer_info[group_key][name] = {\n",
    "                'shape': (rows, cols),\n",
    "                'original_params': rows * cols,\n",
    "                'cost_per_rank': rows + cols,\n",
    "                'max_rank': max_rank,\n",
    "                'importance': importance_score\n",
    "            }\n",
    "\n",
    "    if not grouped_layer_info:\n",
    "        print(\"No compressible layers found or matched with importance_dict. Aborting.\")\n",
    "        return {}, {}\n",
    "\n",
    "    # --- 2. Process Each Group Independently ---\n",
    "    final_ranks = {}\n",
    "    detailed_stats = OrderedDict()\n",
    "    overall_original_params = 0\n",
    "    overall_final_params = 0\n",
    "\n",
    "    for group_name, layer_info in grouped_layer_info.items():\n",
    "        print(f\"\\n--- Processing Group: {group_name} ({len(layer_info)} layers) ---\")\n",
    "        \n",
    "        # --- 2a. Normalize and Smooth Importance (within group) ---\n",
    "        total_importance_group = sum(info['importance'] for info in layer_info.values())\n",
    "        if total_importance_group == 0:\n",
    "            for name in layer_info:\n",
    "                layer_info[name]['final_weight'] = 1.0 / len(layer_info)\n",
    "        else:\n",
    "            for name in layer_info:\n",
    "                normalized_importance = layer_info[name]['importance'] / total_importance_group\n",
    "                layer_info[name]['smoothed_importance'] = normalized_importance ** smoothing_alpha\n",
    "            total_smoothed_importance = sum(info['smoothed_importance'] for info in layer_info.values())\n",
    "            for name in layer_info:\n",
    "                layer_info[name]['final_weight'] = layer_info[name]['smoothed_importance'] / total_smoothed_importance\n",
    "\n",
    "        # --- 2b. Calculate Group Budget ---\n",
    "        total_original_params_group = sum(info['original_params'] for info in layer_info.values())\n",
    "        target_total_params_group = total_original_params_group * compression_ratio\n",
    "        overall_original_params += total_original_params_group\n",
    "        \n",
    "        print(f\"Group Original Params: {total_original_params_group:,}\")\n",
    "        print(f\"Group Target Params:   {int(target_total_params_group):,}\")\n",
    "\n",
    "        # --- 2c. Iterative Rank Allocation (for this group) ---\n",
    "        group_final_ranks = {}\n",
    "        remaining_budget = target_total_params_group\n",
    "        layers_to_process = list(layer_info.keys())\n",
    "        \n",
    "        is_stable = False\n",
    "        while not is_stable and layers_to_process:\n",
    "            is_stable = True\n",
    "            weighted_cost_sum = sum(layer_info[name]['final_weight'] * layer_info[name]['cost_per_rank'] for name in layers_to_process)\n",
    "            if weighted_cost_sum == 0: break\n",
    "            \n",
    "            alloc_const = remaining_budget / weighted_cost_sum\n",
    "            next_layers_to_process = []\n",
    "            \n",
    "            for name in layers_to_process:\n",
    "                info = layer_info[name]\n",
    "                tentative_rank = alloc_const * info['final_weight']\n",
    "                if tentative_rank >= info['max_rank']:\n",
    "                    is_stable = False\n",
    "                    group_final_ranks[name] = info['max_rank']\n",
    "                    remaining_budget -= info['max_rank'] * info['cost_per_rank']\n",
    "                else:\n",
    "                    next_layers_to_process.append(name)\n",
    "            layers_to_process = next_layers_to_process\n",
    "\n",
    "        if layers_to_process:\n",
    "            weighted_cost_sum = sum(layer_info[name]['final_weight'] * layer_info[name]['cost_per_rank'] for name in layers_to_process)\n",
    "            if weighted_cost_sum > 0:\n",
    "                alloc_const = remaining_budget / weighted_cost_sum\n",
    "                for name in layers_to_process:\n",
    "                    group_final_ranks[name] = int(max(1, np.floor(alloc_const * layer_info[name]['final_weight'])))\n",
    "        \n",
    "        final_ranks.update(group_final_ranks)\n",
    "\n",
    "        # --- 2d. Update Stats for Reporting ---\n",
    "        for name, info in layer_info.items():\n",
    "            rank = group_final_ranks.get(name, 0)\n",
    "            new_params = rank * info['cost_per_rank']\n",
    "            overall_final_params += new_params\n",
    "            individual_compression = 1.0 - (new_params / info['original_params']) if info['original_params'] > 0 else 0\n",
    "            detailed_stats[name] = {\n",
    "                \"group\": group_name,\n",
    "                \"shape\": info['shape'],\n",
    "                \"importance\": info['importance'],\n",
    "                \"original_params\": info['original_params'],\n",
    "                \"final_rank_k\": rank,\n",
    "                \"new_params\": new_params,\n",
    "                \"compression\": f\"{individual_compression:.2%}\"\n",
    "            }\n",
    "\n",
    "    # --- 3. Final Report Generation ---\n",
    "    actual_compression_ratio = 1.0 - (overall_final_params / overall_original_params) if overall_original_params > 0 else 0\n",
    "    print(\"\\n--- Overall Compression Results ---\")\n",
    "    print(f\"Target Compression Ratio:   {compression_ratio:.2%}\")\n",
    "    print(f\"Achieved Compression Ratio: {actual_compression_ratio:.2%}\")\n",
    "    print(f\"Original Parameters: {overall_original_params:,}\")\n",
    "    print(f\"Final Parameters:    {int(overall_final_params):,}\")\n",
    "    print(\"-----------------------------------\\n\")\n",
    "\n",
    "    # --- 4. Clean up keys for return ---\n",
    "    final_ranks_clean = {k.replace('.weight', ''): v for k, v in final_ranks.items()}\n",
    "\n",
    "    return final_ranks_clean, detailed_stats\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    MODEL_ID = \"huggyllama/llama-7b\"\n",
    "    COMPRESSION_RATIO = 0.60  # Target: 60% smaller, 40% of original size\n",
    "    SMOOTHING_ALPHA = 0.1# Value between 0 and 1. Closer to 0 = more uniform ranks.\n",
    "\n",
    "    # --- 1. Load Model and Importance Scores ---\n",
    "    print(f\"Loading model: {MODEL_ID}. This may take a while...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "    \n",
    "    # Load the gradients to verify\n",
    "    with open(\"grads/llama7b_grads_out.pt\", \"rb\") as f:\n",
    "        importance_dict = torch.load(f)\n",
    "        \n",
    "    # Calculate the average importance of each layer\n",
    "    importance_avg = OrderedDict()\n",
    "    for layer_name, importance in importance_dict.items():\n",
    "        importance_avg[layer_name] = torch.mean(importance).item()\n",
    "    # --- 2. Run the Algorithm ---\n",
    "    final_ranks, detailed_stats = calculate_truncation_ranks(\n",
    "        model=model,\n",
    "        importance_dict=importance_avg,\n",
    "        compression_ratio=COMPRESSION_RATIO,\n",
    "        smoothing_alpha=SMOOTHING_ALPHA\n",
    "    )\n",
    "\n",
    "    # --- 3. Print Detailed Layer-by-Layer Results ---\n",
    "    if detailed_stats:\n",
    "        # Sort stats by group and then by importance for a structured view\n",
    "        sorted_stats = sorted(detailed_stats.items(), key=lambda item: (item[1]['group'], -item[1]['importance']))\n",
    "        \n",
    "        print(f\"{'Group':<20} {'Layer Name':<45} {'Importance':<12} {'Shape':<15} {'Orig. Params':<15} {'New Rank (k)':<15} {'New Params':<15} {'Compression'}\")\n",
    "        print(\"-\" * 160)\n",
    "        for name, stats in sorted_stats:\n",
    "            print(f\"{stats['group']:<20} {name:<45} {stats['importance']:<12.4f} {str(stats['shape']):<15} {stats['original_params']:,<15} {stats['final_rank_k']:,<15} {stats['new_params']:,<15} {stats['compression']}\")\n",
    "\n",
    "    # --- 4. Clean up ---\n",
    "    del model\n",
    "    gc.collect()\n",
    "    print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42395c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
